#train_data$g <- as.numeric(as.character(train_data$g))
test_data <- testing(data_split)
# Define predictor and response variables
train_x <- train_data[, -which(names(train_data) == 'g')]
train_y <- train_data[['g']]
test_x <- test_data[, -which(names(test_data) == 'g')]
test_y <- test_data[['g']]
classifier_RF = randomForest(x = train_x,
y = train_y,
ntree = 500)
classifier_RF <- randomForest(x = train_x,
y = as.factor(train_y),  # Ensure train_y is a factor
ntree = 500)
# Predicting the Test set results
y_pred <- predict(classifier_RF, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
confusion_mtx
# Plotting model
plot(classifier_RF)
# Importance plot
importance(classifier_RF)
# Variable importance plot
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
install.packages('e1071')
library(e1071)
classifier = svm(formula = train_y~ .,
data = train,
type = 'C-classification',
kernel = 'linear')
library(e1071)
dftrain<-data.frame(train)
classifier = svm(formula = train_y~ .,
data = dftrain,
type = 'C-classification',
kernel = 'linear')
library(e1071)
dftrain_y<-
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'linear')
dftrain
train
library(e1071)
dftrain<-data.frame(train_data)
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'linear')
dftrain<-data.frame(train_data)
dftrain
View(data_split)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
classifier
y_pred = predict(classifier, newdata = test_y[-3])
y_pred = predict(classifier, newdata = test_y)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_y)
test_y
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Importance plot
importance(classifier)
install.packages('e1071')
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Importance plot
importance(classifier)
install.packages("e1071")
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Importance plot
importance(classifier)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
objModel <- train(train[,1:34], train[,35],
method='gbm',
trControl=objControl,
metric = "ROC",
preProc = c("center", "scale"))
objModel <- train(train_x, train_y,
method='gbm',
trControl=objControl,
metric = "ROC",
preProc = c("center", "scale"))
objControl <- trainControl(method='cv', number=3, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)
objModel <- train(train_x, train_y,
method='gbm',
trControl=objControl,
metric = "ROC",
preProc = c("center", "scale"))
# Train GBM model
gbm_model <- gbm(train_y ~ ., data = train_data, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
library(gbm)
# Train GBM model
gbm_model <- gbm(train_y ~ ., data = train_data, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
gbm_model <- gbm(train_y ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
yfactor<-as.factor(train_y)
gbm_model <- gbm(yfactor ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(yfactor ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
# Predicting the Test set results
y_pred <- predict(gbm_model, newdata = test_y, type = "response")
dftrain<-data.frame(train_data)
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(yfactor ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
# Predicting the Test set results
y_pred <- predict(gbm_model, newdata = test_y, type = "response")
dftrain<-data.frame(train_data)
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(yfactor ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
# Predicting the Test set results
y_pred <- predict(gbm_model, newdata = test_y, type = "response")
# Load necessary libraries
library(caret)
library(rsample)
library(party)
library(caTools)
library(randomForest)
# Read data
traindata <- read.csv('/home/deniz/Masaüstü/yükseklisans/ayz/xaifinal/binarydataset/ionosphere.data.csv', sep=',')
# Rename columns
colnames(traindata)[1:34] <- paste0("prop", 1:35)
# Replace 'g' with 1 and 'b' with 0
traindata$g[traindata$g == "g"] <- 1
traindata$g[traindata$g == "b"] <- 0
# Create a data split
set.seed(123)
data_split <- initial_split(traindata, prop = 0.70)
train_data <- training(data_split)
#train_data$g <- as.numeric(as.character(train_data$g))
test_data <- testing(data_split)
# Define predictor and response variables
train_x <- train_data[, -which(names(train_data) == 'g')]
train_y <- train_data[['g']]
test_x <- test_data[, -which(names(test_data) == 'g')]
test_y <- test_data[['g']]
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(yfactor ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
# Load necessary libraries
library(caret)
library(rsample)
library(party)
library(caTools)
library(randomForest)
# Read data
traindata <- read.csv('/home/deniz/Masaüstü/yükseklisans/ayz/xaifinal/binarydataset/ionosphere.data.csv', sep=',')
# Rename columns
colnames(traindata)[1:34] <- paste0("prop", 1:35)
# Replace 'g' with 1 and 'b' with 0
traindata$g[traindata$g == "g"] <- 1
traindata$g[traindata$g == "b"] <- 0
# Create a data split
set.seed(123)
data_split <- initial_split(traindata, prop = 0.70)
train_data <- training(data_split)
#train_data$g <- as.numeric(as.character(train_data$g))
test_data <- testing(data_split)
# Define predictor and response variables
train_x <- train_data[, -which(names(train_data) == 'g')]
train_y <- train_data[['g']]
test_x <- test_data[, -which(names(test_data) == 'g')]
test_y <- test_data[['g']]
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(train_y ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
# Predicting the Test set results
y_pred <- predict(gbm_model, newdata = test_y, type = "response")
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(train_y ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
# Predicting the Test set results
test_dataframe_y<-as.factor(test_y)
y_pred <- predict(gbm_model, newdata = test_dataframe_y, type = "response")
library(gbm)
# Train GBM model
dftrain<-data.frame(train_data)
dftrain$g <- as.factor(dftrain$g)
gbm_model <- gbm(train_y ~ ., data = dftrain, distribution = "bernoulli", n.trees = 500, interaction.depth = 5, shrinkage = 0.01)
colnames(test_dataframe_y)[1:34] <- paste0("prop", 1:34)
install.packages("lightgbm")
install.packages("lightgbm")
library(caret)
library(lightgbm)
dtrain = lgb.Dataset(train_x, label = train_y)
dtest = lgb.Dataset.create.valid(dtrain, data = test_x, label = test_y)
# define parameters
params = list(
objective= 'multiclass',
metric = "multi_error",
num_class= 3
)
# validataion data
valids = list(test = dtest)
# train model
model = lgb.train(params,
dtrain,
nrounds = 100,
valids,
min_data=1,
learning_rate = 1,
early_stopping_rounds = 10)
library(caret)
library(lightgbm)
dtrain = lgb.Dataset(train_x, label = train_y)
dtest = lgb.Dataset.create.valid(dtrain, data = test_x, label = test_y)
# define parameters
params = list(
objective= 'multiclass',
metric = "multi_error",
num_class= 3
)
# validataion data
valids = list(test = dtest)
# train model
model = lgb.train(params,
dtrain,
nrounds = 100,
valids,
min_data=1,
learning_rate = 1,
early_stopping_rounds = 10)
library(caret)
library(lightgbm)
dtrain = lgb.Dataset(train_x, label = train_y)
dtest = lgb.Dataset.create.valid(dtrain, data = test_x, label = test_y)
# define parameters
params = list(
objective= 'multiclass',
metric = "multi_error",
num_class= 3
)
# validataion data
valids = list(test = dtest)
# train model
model = lgb.train(params,
dtrain,
nrounds = 100,
valids)
library(caret)
library(lightgbm)
# Assuming train_x and test_x are data frames, and train_y and test_y are numeric or factor vectors
# Convert train_y and test_y to factor (assuming it's a classification problem)
train_y <- as.factor(train_y)
test_y <- as.factor(test_y)
# Create lgb.Dataset objects
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = as.numeric(train_y))
dtest <- lgb.Dataset.create.valid(dtrain, data = as.matrix(test_x), label = as.numeric(test_y))
# Define parameters
params <- list(
objective = 'multiclass',
metric = 'multi_error',
num_class = 3
)
# Validation data
valids <- list(test = dtest)
# Train the model
model <- lgb.train(params, dtrain, nrounds = 100, valids)
# Print best scores
print(model$best_score)
# Prediction
pred <- predict(model, as.matrix(test_x), reshape = TRUE)
Prediction
# Prediction
pred <- predict(model, as.matrix(test_x))
pred_y <- max.col(pred) - 1
pred_y
plot(classifier, dftrain,)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
plot(classifier, dftrain,)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial')
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
plot(classifier, dftrain)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial',
cost = 10,
scale = FALSE)
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
plot(classifier, dftrain)
dftrain
library(ggplot2)
# Assuming dftrain has been loaded with your data
# Combine 'g' with the data for plotting
dfplot <- cbind(dftrain, g = as.factor(dftrain$g))
# Plot for class 0
ggplot(dfplot[dfplot$g == 0, ], aes(x = prop1, y = prop2, color = g)) +
geom_point() +
ggtitle("Scatter Plot for Class 0")
library(ggplot2)
# Assuming dftrain has been loaded with your data
# Check for duplicate column names
dup_cols <- names(dftrain)[duplicated(names(dftrain))]
if (length(dup_cols) > 0) {
cat("Duplicate column names found. Renaming them.\n")
for (col in dup_cols) {
names(dftrain)[which(names(dftrain) == col)] <- paste0(col, "_dup")
}
}
# Combine 'g' with the data for plotting
dfplot <- cbind(dftrain, g = as.factor(dftrain$g))
# Plot for class 0
ggplot(dfplot[dfplot$g == 0, ], aes(x = prop1, y = prop2, color = g)) +
geom_point() +
ggtitle("Scatter Plot for Class 0")
plot(classifier, dftrain)
plot(classifier, dftrain,g~prop1)
plot(classifier, dftrain,g)
plot(classifier, dftrain,g)
plot(classifier, dftrain,prop1)
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop1, x4 = prop2))
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop1, x4 = prop2))
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop4, x4 = prop2))
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop11, x4 = prop2))
prop11<-dftrain$prop4
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop11, x4 = prop2))
prop22<-dftrain$prop22
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop11, x4 = prop22))
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop11, x4 = prop22))
plot(classifier, dftrain,g ~ prop3, slice = list(x3 = prop11, x4 = prop22))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
plot(classifier, dftrain, slice = list("1","0"))
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier = svm(formula =g~ .,
data = dftrain,
type = 'C-classification',
kernel = 'radial',
cost = 10,
scale = FALSE)
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
plot(classifier, dftrain)
plot(classifier, dftrain)
plot(classifier, dftrain)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier <- svm(g ~ ., data = dftrain, type = 'C-classification', kernel = 'radial', cost = 10, scale = FALSE)
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Plotting the SVM classification
plot(classifier, dftrain)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier <- svm(g ~ ., data = dftrain, type = 'C-classification', kernel = 'radial', cost = 10, scale = FALSE)
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Plotting the SVM classification
plot(classifier, dftrain)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier <- svm(g ~ ., data = dftrain, type = 'C-classification', kernel = 'radial', cost = 10, scale = FALSE)
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Plotting the SVM classification
plot(classifier, dftrain)
dftrain
head(dftrain)
library(e1071)
dftrain<-data.frame(train_data)
dftrain <- dftrain[, sapply(dftrain, sd) != 0]
classifier <- svm(g ~ ., data = dftrain, type = 'C-classification', kernel = 'radial', cost = 10, scale = FALSE)
test_y <- as.factor(test_y)
# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_x)
# Confusion Matrix
confusion_mtx <- table(test_y, y_pred)
print(confusion_mtx)
# Plotting the SVM classification
plot(classifier, dftrain)
